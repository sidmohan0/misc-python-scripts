{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMZjy50ATtbVhCvtZQv5Ys6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sidmohan0/misc-python-scripts/blob/main/whatsappcleanup.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Overview\n",
        "This repository contains a Python script for cleaning and extracting semantic topics from a WhatsApp chat archive text file. The script takes in the text file, preprocesses the data, tokenizes the text, and extracts relevant words and phrases based on a list of topics provided by the user. The final output is a CSV or Pandas DataFrame that can be fed into a semantic search engine for analysis and exploration.\n",
        "\n",
        "# Requirements\n",
        "Python 3.x\n",
        "Pandas\n",
        "NLTK\n",
        "Regex\n",
        "Usage\n",
        "To use the script, clone this repository and navigate to the directory in your terminal or command prompt. Run the following command:\n",
        "\n",
        "\n",
        "```\n",
        "python main.py [input_file] [topics_file] [output_file]\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "Where [input_file] is the name of your WhatsApp chat archive text file, [topics_file] is the name of the file containing the list of topics, and [output_file] is the name of the output CSV or DataFrame.\n",
        "\n",
        "# Contributions\n",
        "Contributions are welcome! If you'd like to make changes or improvements to the code, please open a pull request.\n",
        "\n",
        "# License\n",
        "This project is licensed under the MIT License. See LICENSE for details."
      ],
      "metadata": {
        "id": "rZLZV95QjTXq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialization\n",
        "\n",
        "Let's start by installing the required libraries:"
      ],
      "metadata": {
        "id": "0JwmRkalmJf8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "\n"
      ],
      "metadata": {
        "id": "0V1cyP0nmIBN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xDjFsG4eoQpf",
        "outputId": "434ab29f-4d5c-4c15-847a-6b0ece92f228"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this notebook I'm going to use a text file, chats.txt, which contains several lines of conversation between three fictional people (Sarah, Tom, John) and the conversations span a mix of fully formatted (with timestamp), just the name of the person (i.e. \"John: \") or empty lines or random characters. "
      ],
      "metadata": {
        "id": "tveMC3e0msJe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wZ-YgkYstTpH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5O9oE0U2vEJ2",
        "outputId": "b6c069eb-fc47-423c-99e5-e37a674a4736"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "print(stopwords.words('english'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jt5dK1a9RJJ",
        "outputId": "c98a2340-9bc3-4685-e94d-ffc3a55c85aa"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I3NJ7Rgzi374"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def preprocess(text):\n",
        "    # Step 1: Lowercase the text\n",
        "    text = text.lower()\n",
        "    \n",
        "    # Step 2: Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "    \n",
        "    # Step 3: Remove stop words\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    tokens = [x for x in tokens if x not in stop_words]\n",
        "    \n",
        "    # Step 4: Remove punctuation and special characters\n",
        "    tokens = [re.sub(r'[^\\w\\s]', '', token) for token in tokens]\n",
        "    \n",
        "    # Step 5: Identify proper nouns and remove them\n",
        "    pos_tags = nltk.pos_tag(tokens)\n",
        "    tokens = [token for token, pos in pos_tags if pos != 'NNP']\n",
        "    \n",
        "    # Step 6: Lemmatize the tokens\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    \n",
        "    return tokens\n",
        "\n",
        "\n",
        "file_name = 'chats.txt'\n",
        "names = set()\n",
        "\n",
        "with open(file_name, 'r') as file:\n",
        "  text = file.read()\n",
        "  lines = text.split('\\n')\n",
        "  \n",
        "  for line in lines:\n",
        "    if line.startswith('['):\n",
        "      split_line = line.split(']')\n",
        "      if len(split_line) >= 2:\n",
        "        names.add(split_line[1].split(':')[0].strip())\n",
        "\n",
        "    elif line.count(':') == 1:\n",
        "        names.add(line.split(':')[0].strip())\n",
        "    \n",
        "print(\"Names:\")\n",
        "print(names)\n",
        "\n",
        "preprocessed_text = preprocess(text)\n",
        "print(\"Preprocessed text:\")\n",
        "print(preprocessed_text)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "names_lower = [item.lower() for item in names]\n",
        "print(names_lower)\n"
      ],
      "metadata": {
        "id": "sZQzDRpgzLHs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_names(lst):\n",
        "    return [x for x in lst if x not in names_lower]\n",
        "\n",
        "processed_list_names = remove_names(preprocessed_text)\n",
        "\n",
        "print(preprocessed_text)\n",
        "print(processed_list_names)"
      ],
      "metadata": {
        "id": "ll8erukZyHwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def remove_blank_spaces(lst):\n",
        "    return [x.strip() for x in lst if x.strip()]\n",
        "processed_list_no_names_no_spaces = remove_blank_spaces(processed_list_names)\n",
        "\n",
        "print(preprocessed_text)\n",
        "print(processed_list_names)\n",
        "print(processed_list_no_names_no_spaces)"
      ],
      "metadata": {
        "id": "y5vsdaLTeKvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_short_items(lst):\n",
        "    return [x for x in lst if len(x) > 2]\n",
        "processed_list_no_names_no_spaces_no_short_items = remove_blank_spaces(processed_list_no_names_no_spaces)\n",
        "\n",
        "print(preprocessed_text)\n",
        "print(processed_list_names)\n",
        "print(processed_list_no_names_no_spaces)\n",
        "print(processed_list_no_names_no_spaces_no_short_items)"
      ],
      "metadata": {
        "id": "g_Naj_wbeOzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_length_of_strings(list_of_strings):\n",
        "    for string in list_of_strings:\n",
        "        print(len(string))\n",
        "\n",
        "print_length_of_strings(processed_list_no_names_no_spaces_no_short_items)"
      ],
      "metadata": {
        "id": "xDA1MfqVg8xz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_short_strings(list_of_strings):\n",
        "    for string in list_of_strings:\n",
        "        if len(string) < 2:\n",
        "            list_of_strings.remove(string)\n",
        "    return list_of_strings\n",
        "\n",
        "print(remove_short_strings(processed_list_no_names_no_spaces_no_short_items))"
      ],
      "metadata": {
        "id": "vNwM8EJBh2Sa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_list = processed_list_no_names_no_spaces_no_short_items"
      ],
      "metadata": {
        "id": "1Y3txLAeh_8g"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When working with natural language processing (NLP) and large amounts of text data, the dimensionality of the data can become quite high. This can make it difficult to process and analyze the data efficiently. To address this issue, we often use a technique called stemming or lemmatizing to reduce the dimensionality of the data.\n",
        "\n",
        "Stemming and lemmatizing are processes that aim to reduce words to their base or root form. For example, words like \"running,\" \"runner,\" and \"ran\" would all be reduced to their base form \"run.\" By reducing words to their base form, we can better identify and group together words that have a similar meaning, even if they are written differently.\n",
        "\n",
        "The two most common techniques for stemming or lemmatizing in NLP are the PorterStemmer and the WordNetLemmatizer. The PorterStemmer is a rule-based approach to stemming that uses a set of rules to reduce words to their base form. The WordNetLemmatizer, on the other hand, uses a dictionary to look up words and reduce them to their base form.\n",
        "\n",
        "There are trade-offs to both techniques. The PorterStemmer is a relatively simple and fast approach, but it may not always produce the most accurate results. The WordNetLemmatizer, on the other hand, is a more sophisticated approach that can produce more accurate results, but it may also be slower and more resource-intensive.\n",
        "\n",
        "Ultimately, the choice between stemming and lemmatizing will depend on the specific requirements of your NLP project. If accuracy is a top priority, lemmatizing may be the way to go. If speed and simplicity are more important, stemming may be the better choice.\n",
        "\n",
        "In this case, I'm going to go ahead with lemmatizing.  in this case since i'm running on my own compute, I'll spluge and go with WordNetLemmatizer. "
      ],
      "metadata": {
        "id": "6xde30G8qcta"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# lemmatize words in a list\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in final_list]\n",
        "\n",
        "# Create a dictionary to store the themes and their counts\n",
        "themes = {}\n",
        "\n",
        "# Loop through each token in the preprocessed text\n",
        "for token in final_list:\n",
        "    # If the token is already a key in the themes dictionary, increment its value by 1\n",
        "    if token in themes:\n",
        "        themes[token] += 1\n",
        "    # Otherwise, add the token to the themes dictionary with a value of 1\n",
        "    else:\n",
        "        themes[token] = 1\n",
        "\n",
        "# Sort the themes by their count, in descending order\n",
        "sorted_themes = sorted(themes.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Print the top 5 most common themes\n",
        "print(\"Top 5 themes:\")\n",
        "for i in range(5):\n",
        "    print(\"{}. {} ({} occurrences)\".format(i + 1, sorted_themes[i][0], sorted_themes[i][1]))\n",
        "\n",
        "# Extract and categorize important dates and events\n",
        "# (TODO: implement code to extract and categorize dates)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "olvGP1UXqomT",
        "outputId": "e0b03f4e-5bc4-46d5-8edc-5cfbcb988838"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 themes:\n",
            "1. great (11 occurrences)\n",
            "2. 01012022 (11 occurrences)\n",
            "3. ll (7 occurrences)\n",
            "4. one (7 occurrences)\n",
            "5. hey (6 occurrences)\n"
          ]
        }
      ]
    }
  ]
}